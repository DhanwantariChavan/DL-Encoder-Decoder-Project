# -*- coding: utf-8 -*-
"""Headline Generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lLjCDtPfqTFeJdJvySxnCGhcMgTAXzq_

Group Members:

202201040143 - Dhanwantari Chavan

202201040145 - Sai Gulve

202201040127 - Siddhi Bambal

Batch : DL3

Lab Assignment 6

 Aim : Comparative Study of Encoder-Decoder Architectures with Attention Mechanisms
"""

# Install required packages
!pip install nltk rouge numpy pandas torch matplotlib tqdm

"""## Headline Generation using Encoder-Decoder Architecture without Attention
## Task 2 Implementation - LSTM/GRU-based encoder-decoder

"""

# Headline Generation with Encoder-Decoder Architecture (No Attention)
# Based on the paper: "Generating News Headlines with Recurrent Neural Networks" by Konstantin Lopyrev

# Import required libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
from nltk.tokenize import word_tokenize
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from nltk.translate.meteor_score import meteor_score
from rouge import Rouge
import re
import string
import time
import os
import io
from google.colab import files
from collections import Counter
from tqdm.notebook import tqdm

# Download necessary NLTK packages
nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('wordnet')

# Step 1: Upload and Read the Dataset
print("Please upload your news headlines dataset CSV file...")
uploaded = files.upload()

# Get the uploaded file name
file_name = list(uploaded.keys())[0]
print(f"Uploaded file: {file_name}")

df = pd.read_csv(io.BytesIO(uploaded[file_name]), encoding='ISO-8859-1')

# Display dataset information
print(f"Dataset shape: {df.shape}")
print("\nDataset columns:")
print(df.columns.tolist())
print("\nPreview of the dataset:")
print(df.head())

# Step 2: Data Preprocessing
class NewsDataset:
  def __init__(self, df, text_col='text', headline_col='headlines', max_article_len=400, max_headline_len=20):
    self.df = df
    self.text_col = text_col
    self.headline_col = headline_col
    self.max_article_len = max_article_len
    self.max_headline_len = max_headline_len

    # Special tokens
    self.PAD_token = 0
    self.SOS_token = 1
    self.EOS_token = 2
    self.UNK_token = 3

    # Extract texts and headlines
    print("Extracting articles and headlines...")
    self.articles = df[text_col].fillna("").tolist()
    self.headlines = df[headline_col].fillna("").tolist()

    # Clean and tokenize
    print("Cleaning and tokenizing data...")
    self.cleaned_articles = [self.clean_text(text) for text in tqdm(self.articles)]
    self.cleaned_headlines = [self.clean_text(headline) for headline in tqdm(self.headlines)]

    self.tokenized_articles = [self.tokenize(text) for text in tqdm(self.cleaned_articles)]
    self.tokenized_headlines = [self.tokenize(headline) for headline in tqdm(self.cleaned_headlines)]

    # Build vocabularies
    print("Building vocabularies...")
    self.build_vocab()

    # Convert tokens to indices
    print("Converting tokens to indices...")
    self.article_indices = [self.tokens_to_indices(tokens, self.article_word2idx)
                           for tokens in tqdm(self.tokenized_articles)]

    self.headline_indices = [[self.SOS_token] +
                           self.tokens_to_indices(tokens, self.headline_word2idx) +
                           [self.EOS_token]
                           for tokens in tqdm(self.tokenized_headlines)]

    # Create datasets
    print("Creating train/validation/test splits...")
    self.create_datasets()

  def clean_text(self, text):
    """Clean the text by removing special characters and extra spaces"""
    if not isinstance(text, str):
      return ""

    # Convert to lowercase
    text = text.lower()

    # Remove URLs
    text = re.sub(r'http\S+', '', text)

    # Remove punctuation and numbers
    text = re.sub(f'[{string.punctuation}]', ' ', text)
    text = re.sub(r'\d+', '', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

  def tokenize(self, text):
    """Tokenize the text"""
    return word_tokenize(text)

  def build_vocab(self, min_freq=2):
    """Build vocabulary for articles and headlines"""
    # Count word frequencies
    article_counter = Counter()
    headline_counter = Counter()

    for tokens in self.tokenized_articles:
      article_counter.update(tokens)

    for tokens in self.tokenized_headlines:
      headline_counter.update(tokens)

    # Create word to index mappings
    # Reserve indices for special tokens: PAD(0), SOS(1), EOS(2), UNK(3)
    self.article_word2idx = {'<pad>': self.PAD_token, '<sos>': self.SOS_token,
                            '<eos>': self.EOS_token, '<unk>': self.UNK_token}
    self.headline_word2idx = {'<pad>': self.PAD_token, '<sos>': self.SOS_token,
                             '<eos>': self.EOS_token, '<unk>': self.UNK_token}

    # Add words that appear at least min_freq times
    for word, count in article_counter.items():
      if count >= min_freq:
        self.article_word2idx[word] = len(self.article_word2idx)

    for word, count in headline_counter.items():
      if count >= min_freq:
        self.headline_word2idx[word] = len(self.headline_word2idx)

    # Create index to word mappings
    self.article_idx2word = {idx: word for word, idx in self.article_word2idx.items()}
    self.headline_idx2word = {idx: word for word, idx in self.headline_word2idx.items()}

    # Store vocabulary sizes
    self.article_vocab_size = len(self.article_word2idx)
    self.headline_vocab_size = len(self.headline_word2idx)

    print(f"Article vocabulary size: {self.article_vocab_size}")
    print(f"Headline vocabulary size: {self.headline_vocab_size}")

  def tokens_to_indices(self, tokens, word2idx):
    """Convert tokens to indices"""
    return [word2idx.get(token, self.UNK_token) for token in tokens]

  def indices_to_tokens(self, indices, idx2word):
    """Convert indices to tokens"""
    return [idx2word.get(idx, '<unk>') for idx in indices]

  def pad_sequences(self, sequences, max_len):
    """Pad sequences to max_len"""
    padded_seqs = []
    for seq in sequences:
      if len(seq) < max_len:
        padded_seq = seq + [self.PAD_token] * (max_len - len(seq))
      else:
        padded_seq = seq[:max_len]
      padded_seqs.append(padded_seq)
    return padded_seqs

  def create_datasets(self, train_ratio=0.8, val_ratio=0.1):
    """Create train, validation, and test datasets"""
    # Shuffle indices
    indices = list(range(len(self.article_indices)))
    np.random.seed(42)
    np.random.shuffle(indices)

    # Split indices
    train_size = int(train_ratio * len(indices))
    val_size = int(val_ratio * len(indices))

    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size+val_size]
    test_indices = indices[train_size+val_size:]

    # Create datasets
    self.train_dataset = HeadlineDataset(
      [self.article_indices[i] for i in train_indices],
      [self.headline_indices[i] for i in train_indices],
      self.max_article_len,
      self.max_headline_len
    )

    self.val_dataset = HeadlineDataset(
      [self.article_indices[i] for i in val_indices],
      [self.headline_indices[i] for i in val_indices],
      self.max_article_len,
      self.max_headline_len
    )

    self.test_dataset = HeadlineDataset(
      [self.article_indices[i] for i in test_indices],
      [self.headline_indices[i] for i in test_indices],
      self.max_article_len,
      self.max_headline_len
    )

    print(f"Train samples: {len(self.train_dataset)}")
    print(f"Validation samples: {len(self.val_dataset)}")
    print(f"Test samples: {len(self.test_dataset)}")

class HeadlineDataset(Dataset):
  """PyTorch Dataset for headline generation task"""
  def __init__(self, article_indices, headline_indices, max_article_len, max_headline_len):
    self.article_indices = article_indices
    self.headline_indices = headline_indices
    self.max_article_len = max_article_len
    self.max_headline_len = max_headline_len

    # Pad sequences
    self.padded_articles = [
      seq[:self.max_article_len] + [0] * max(0, self.max_article_len - len(seq))
      for seq in self.article_indices
    ]

    self.padded_headlines = [
      seq[:self.max_headline_len] + [0] * max(0, self.max_headline_len - len(seq))
      for seq in self.headline_indices
    ]

  def __len__(self):
    return len(self.padded_articles)

  def __getitem__(self, idx):
    return (
      torch.tensor(self.padded_articles[idx], dtype=torch.long),
      torch.tensor(self.padded_headlines[idx], dtype=torch.long)
    )

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')  # WordNet lemmatizer dependency

# Process dataset
print("Processing dataset...")
news_data = NewsDataset(df, text_col='text', headline_col='headlines')

# Create DataLoaders
batch_size = 32
train_loader = DataLoader(news_data.train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(news_data.val_dataset, batch_size=batch_size)
test_loader = DataLoader(news_data.test_dataset, batch_size=batch_size)

# Step 3: Model Implementation
class EncoderLSTM(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size, num_layers=1, dropout=0.5):
    super(EncoderLSTM, self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers

    self.embedding = nn.Embedding(input_size, embedding_size)
    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers,
                        dropout=dropout if num_layers > 1 else 0, batch_first=True)
    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    # x: (batch_size, seq_len)
    embedded = self.dropout(self.embedding(x))  # (batch_size, seq_len, embedding_size)

    outputs, (hidden, cell) = self.lstm(embedded)
    # outputs: (batch_size, seq_len, hidden_size)
    # hidden: (num_layers, batch_size, hidden_size)
    # cell: (num_layers, batch_size, hidden_size)

    return outputs, hidden, cell

class DecoderLSTM(nn.Module):
  def __init__(self, output_size, embedding_size, hidden_size, num_layers=1, dropout=0.5):
    super(DecoderLSTM, self).__init__()
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.num_layers = num_layers

    self.embedding = nn.Embedding(output_size, embedding_size)
    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers,
                        dropout=dropout if num_layers > 1 else 0, batch_first=True)
    self.fc = nn.Linear(hidden_size, output_size)
    self.dropout = nn.Dropout(dropout)

  def forward(self, x, hidden, cell):
    # x: (batch_size)
    x = x.unsqueeze(1)  # (batch_size, 1)

    embedded = self.dropout(self.embedding(x))  # (batch_size, 1, embedding_size)
    output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
    # output: (batch_size, 1, hidden_size)

    prediction = self.fc(output.squeeze(1))  # (batch_size, output_size)

    return prediction, hidden, cell

class Seq2SeqModel(nn.Module):
  def __init__(self, encoder, decoder, device):
    super(Seq2SeqModel, self).__init__()
    self.encoder = encoder
    self.decoder = decoder
    self.device = device

  def forward(self, source, target, teacher_forcing_ratio=0.5):
    # source: (batch_size, source_len)
    # target: (batch_size, target_len)

    batch_size = source.shape[0]
    target_len = target.shape[1]
    target_vocab_size = self.decoder.output_size

    # Tensor to store decoder outputs
    outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)

    # Encode the source sequence
    _, hidden, cell = self.encoder(source)

    # First decoder input is the <SOS> token (index 1)
    decoder_input = target[:, 0]

    for t in range(1, target_len):
      # Get decoder output for current time step
      output, hidden, cell = self.decoder(decoder_input, hidden, cell)

      # Store output
      outputs[:, t, :] = output

      # Decide whether to use teacher forcing
      teacher_force = torch.rand(1).item() < teacher_forcing_ratio

      # Get the highest predicted token
      top1 = output.argmax(1)

      # Use either target token or predicted token as the next input
      decoder_input = target[:, t] if teacher_force else top1

    return outputs

# Step 4: Training functions
def train_epoch(model, dataloader, optimizer, criterion, clip=1.0):
  model.train()
  epoch_loss = 0

  for batch_idx, (src, tgt) in enumerate(tqdm(dataloader)):
    src = src.to(device)
    tgt = tgt.to(device)

    optimizer.zero_grad()

    output = model(src, tgt)

    # output: (batch_size, target_len, output_dim)
    # tgt: (batch_size, target_len)

    output_dim = output.shape[-1]

    # Exclude the first token (SOS) in the output calculation
    output = output[:, 1:].reshape(-1, output_dim)
    tgt = tgt[:, 1:].reshape(-1)

    loss = criterion(output, tgt)
    loss.backward()

    # Clip gradients to prevent exploding gradients
    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

    optimizer.step()

    epoch_loss += loss.item()

    # Print progress
    if batch_idx % 100 == 0 and batch_idx > 0:
      print(f"  Batch {batch_idx}/{len(dataloader)} | Loss: {loss.item():.4f}")

  return epoch_loss / len(dataloader)

def evaluate(model, dataloader, criterion):
  model.eval()
  epoch_loss = 0

  with torch.no_grad():
    for src, tgt in dataloader:
      src = src.to(device)
      tgt = tgt.to(device)

      # Feed target as input but with 0 teacher forcing
      output = model(src, tgt, teacher_forcing_ratio=0)

      output_dim = output.shape[-1]

      # Exclude the first token (SOS) in the output calculation
      output = output[:, 1:].reshape(-1, output_dim)
      tgt = tgt[:, 1:].reshape(-1)

      loss = criterion(output, tgt)

      epoch_loss += loss.item()

  return epoch_loss / len(dataloader)

def generate_headline(model, article, max_len=20):
  model.eval()

  with torch.no_grad():
    # Convert article to tensor and move to device
    article_tensor = article.unsqueeze(0).to(device)

    # Encode the article
    _, hidden, cell = model.encoder(article_tensor)

    # Start with <SOS> token
    decoder_input = torch.tensor([1]).to(device)  # SOS token index = 1

    headline_indices = [1]  # Start with SOS token

    for _ in range(max_len):
      # Decode one token at a time
      output, hidden, cell = model.decoder(decoder_input, hidden, cell)

      # Get most probable token
      top_token = output.argmax(1).item()

      # Add token to the headline
      headline_indices.append(top_token)

      # Stop if EOS token
      if top_token == 2:  # EOS token index = 2
        break

      # Update decoder input
      decoder_input = torch.tensor([top_token]).to(device)

    return headline_indices

def calculate_metrics(model, test_loader, idx2word):
  model.eval()

  references = []  # Ground truth headlines
  hypotheses = []  # Generated headlines

  with torch.no_grad():
    for src, tgt in tqdm(test_loader, desc="Calculating metrics"):
      src = src.to(device)

      for i in range(src.shape[0]):
        # Generate headline for each article in the batch
        predicted_indices = generate_headline(model, src[i])

        # Convert to words, removing special tokens
        predicted_tokens = [idx2word.get(idx, '<unk>') for idx in predicted_indices
                           if idx > 3]  # Skip PAD, SOS, EOS, UNK

        # Get reference headline
        reference_indices = tgt[i].tolist()
        reference_tokens = [[idx2word.get(idx, '<unk>') for idx in reference_indices
                           if idx > 3]]  # Skip PAD, SOS, EOS, UNK

        # Add to lists
        hypotheses.append(predicted_tokens)
        references.append(reference_tokens)
         # Calculate BLEU-1 to BLEU-4
  bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))
  bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))
  bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))
  bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))

  # Calculate ROUGE
  rouge = Rouge()

  # Format for ROUGE
  hyp_texts = [' '.join(h) for h in hypotheses]
  ref_texts = [' '.join(r[0]) for r in references]

  # If there are empty predictions or references, handle them
  valid_pairs = [(h, r) for h, r in zip(hyp_texts, ref_texts) if h and r]
  if not valid_pairs:
    return {
      'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
      'ROUGE-1': 0, 'ROUGE-2': 0, 'ROUGE-L': 0
    }

  hyp_valid, ref_valid = zip(*valid_pairs)

  try:
    rouge_scores = rouge.get_scores(hyp_valid, ref_valid, avg=True)

    return {
      'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
      'ROUGE-1': rouge_scores['rouge-1']['f'],
      'ROUGE-2': rouge_scores['rouge-2']['f'],
      'ROUGE-L': rouge_scores['rouge-l']['f']
    }
  except Exception as e:
    print(f"Error calculating ROUGE: {e}")
    return {
      'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
      'ROUGE-1': 0, 'ROUGE-2': 0, 'ROUGE-L': 0
    }

# Step 5: Model Initialization and Training
# Model hyperparameters
input_size = news_data.article_vocab_size
output_size = news_data.headline_vocab_size
embedding_size = 256
hidden_size = 512
num_layers = 2
dropout = 0.5

# Initialize encoder and decoder
encoder = EncoderLSTM(input_size, embedding_size, hidden_size, num_layers, dropout)
decoder = DecoderLSTM(output_size, embedding_size, hidden_size, num_layers, dropout)

device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# Create the Seq2Seq model
model = Seq2SeqModel(encoder, decoder, device).to(device)

# Print model summary
print(model)

# Set up the loss function and optimizer
pad_idx = 0  # PAD token index
criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)
optimizer = optim.Adam(model.parameters())

# Training parameters
n_epochs = 20
clip = 1.0
best_valid_loss = float('inf')

# Lists to store losses
train_losses = []
val_losses = []

# To store training time
start_time = time.time()

# Training loop
print("Starting training...")
for epoch in range(n_epochs):
  # Train
  train_loss = train_epoch(model, train_loader, optimizer, criterion, clip)
  train_losses.append(train_loss)

  # Evaluate
  val_loss = evaluate(model, val_loader, criterion)
  val_losses.append(val_loss)

  # Save best model
  if val_loss < best_valid_loss:
    best_valid_loss = val_loss
    torch.save(model.state_dict(), 'headline_generator_no_attention.pt')
    print(f"Model saved at epoch {epoch+1}")

  # Print progress
  print(f"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

# Calculate training time
training_time = time.time() - start_time
print(f"Training completed in {training_time:.2f} seconds")

# Step 6: Evaluation
# Load the best model
model.load_state_dict(torch.load('headline_generator_no_attention.pt'))

# Calculate inference time
inference_start_time = time.time()
metrics = calculate_metrics(model, test_loader, news_data.headline_idx2word)
inference_time = time.time() - inference_start_time

# Print metrics
print("\nEvaluation Metrics:")
for metric, value in metrics.items():
  print(f"{metric}: {value:.4f}")

print(f"\nInference time: {inference_time:.2f} seconds")

# Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.savefig('loss_curves_no_attention.png')
plt.show()

# Step 7: Generate sample headlines
def display_sample_predictions(model, test_loader, article_idx2word, headline_idx2word, num_samples=5):
  model.eval()

  samples = []

  with torch.no_grad():
    for src, tgt in test_loader:
      src = src.to(device)

      for i in range(min(num_samples, src.shape[0])):
        # Get the article tokens
        article_indices = src[i].tolist()
        article_tokens = [article_idx2word.get(idx, '<unk>') for idx in article_indices if idx > 0]
        article_text = ' '.join(article_tokens[:50]) + '...'  # Show first 50 tokens only

        # Get the reference headline
        reference_indices = tgt[i].tolist()
        reference_tokens = [headline_idx2word.get(idx, '<unk>') for idx in reference_indices if idx > 0 and idx != 2]  # Skip PAD, SOS, EOS
        reference_text = ' '.join(reference_tokens)

        # Generate headline
        predicted_indices = generate_headline(model, src[i])
        predicted_tokens = [headline_idx2word.get(idx, '<unk>') for idx in predicted_indices if idx > 0 and idx != 2]  # Skip PAD, SOS, EOS
        predicted_text = ' '.join(predicted_tokens)

        samples.append({
          'article': article_text,
          'reference': reference_text,
          'prediction': predicted_text
        })

        if len(samples) >= num_samples:
          break

      if len(samples) >= num_samples:
        break

  # Display samples
  for i, sample in enumerate(samples):
    print(f"\nSample {i+1}:")
    print(f"Article: {sample['article']}")
    print(f"Reference: {sample['reference']}")
    print(f"Prediction: {sample['prediction']}")

# Display sample predictions
print("\nSample Predictions:")
display_sample_predictions(model, test_loader, news_data.article_idx2word, news_data.headline_idx2word)

# Step 8: Save model complexity and training statistics
model_stats = {
  'Model': 'LSTM Encoder-Decoder (No Attention)',
  'Encoder Parameters': sum(p.numel() for p in encoder.parameters()),
  'Decoder Parameters': sum(p.numel() for p in decoder.parameters()),
  'Total Parameters': sum(p.numel() for p in model.parameters()),
  'Training Time (s)': training_time,
  'Inference Time (s)': inference_time,
  'BLEU-1': metrics['BLEU-1'],
  'BLEU-4': metrics['BLEU-4'],
  'ROUGE-1': metrics['ROUGE-1'],
  'ROUGE-L': metrics['ROUGE-L']
}

# Create a DataFrame and save to CSV
stats_df = pd.DataFrame([model_stats])
stats_df.to_csv('model_stats_no_attention.csv', index=False)
print("\nModel statistics saved to 'model_stats_no_attention.csv'")

print("\nTask 2 completed: Encoder-Decoder without Attention implemented and evaluated.")

# Headline Generation with Encoder-Decoder Architecture (With Attention)
# Based on the paper: "Generating News Headlines with Recurrent Neural Networks" by Konstantin Lopyrev

"""## Task 3: Encoder-Decoder with Attention"""

# Step 3: Model Implementation with Attention
class EncoderLSTM(nn.Module):
  def __init__(self, input_size, embedding_size, hidden_size, num_layers=1, dropout=0.5):
    super(EncoderLSTM, self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers

    self.embedding = nn.Embedding(input_size, embedding_size)
    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers=num_layers,
                        dropout=dropout if num_layers > 1 else 0, batch_first=True, bidirectional=True)
    self.fc = nn.Linear(hidden_size * 2, hidden_size)  # Reduce bidirectional output to hidden_size
    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    # x: (batch_size, seq_len)
    embedded = self.dropout(self.embedding(x))  # (batch_size, seq_len, embedding_size)

    # Pass through LSTM
    outputs, (hidden, cell) = self.lstm(embedded)
    # outputs: (batch_size, seq_len, hidden_size*2)
    # hidden: (num_layers*2, batch_size, hidden_size)
    # cell: (num_layers*2, batch_size, hidden_size)

    # Combine forward and backward hidden states
    hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)
    hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)
    hidden = self.fc(hidden)  # (num_layers, batch_size, hidden_size)

    cell = cell.view(self.num_layers, 2, -1, self.hidden_size)
    cell = torch.cat((cell[:, 0, :, :], cell[:, 1, :, :]), dim=2)
    cell = self.fc(cell)  # (num_layers, batch_size, hidden_size)

    return outputs, hidden, cell

class Attention(nn.Module):
  def __init__(self, enc_hidden_size, dec_hidden_size):
    super(Attention, self).__init__()

    # Bidirectional encoder outputs have hidden_size*2 dimensions
    self.attn = nn.Linear((enc_hidden_size * 2) + dec_hidden_size, dec_hidden_size)
    self.v = nn.Linear(dec_hidden_size, 1, bias=False)

  def forward(self, hidden, encoder_outputs, mask=None):
    # hidden: (batch_size, dec_hidden_size)
    # encoder_outputs: (batch_size, src_len, enc_hidden_size*2)

    batch_size = encoder_outputs.shape[0]
    src_len = encoder_outputs.shape[1]

    # Repeat hidden across source length
    hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)

    # Calculate energy
    energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
    # energy: (batch_size, src_len, dec_hidden_size)

    attention = self.v(energy).squeeze(2)
    # attention: (batch_size, src_len)

    # Apply mask if provided (to ignore padding)
    if mask is not None:
      attention = attention.masked_fill(mask == 0, -1e10)

    # Apply softmax to get attention weights
    attention_weights = F.softmax(attention, dim=1)
    # attention_weights: (batch_size, src_len)

    # Calculate context vector
    context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)
    # context: (batch_size, 1, enc_hidden_size*2)
    context = context.squeeze(1)
    # context: (batch_size, enc_hidden_size*2)

    return context, attention_weights

class AttentionDecoderLSTM(nn.Module):
  def __init__(self, output_size, embedding_size, enc_hidden_size, dec_hidden_size, num_layers=1, dropout=0.5):
    super(AttentionDecoderLSTM, self).__init__()
    self.output_size = output_size
    self.dec_hidden_size = dec_hidden_size
    self.num_layers = num_layers

    self.embedding = nn.Embedding(output_size, embedding_size)
    self.attention = Attention(enc_hidden_size, dec_hidden_size)

    # Input to LSTM will be embedding + context vector
    self.lstm = nn.LSTM(embedding_size + (enc_hidden_size * 2), dec_hidden_size,
                       num_layers=num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)

    # Output layer
    self.fc_out = nn.Linear((enc_hidden_size * 2) + dec_hidden_size + embedding_size, output_size)
    self.dropout = nn.Dropout(dropout)

  def forward(self, x, hidden, cell, encoder_outputs, mask=None):
    # x: (batch_size)
    # hidden: (num_layers, batch_size, dec_hidden_size)
    # cell: (num_layers, batch_size, dec_hidden_size)
    # encoder_outputs: (batch_size, src_len, enc_hidden_size*2)

    x = x.unsqueeze(1)  # (batch_size, 1)
    embedded = self.dropout(self.embedding(x))  # (batch_size, 1, embedding_size)

    # Use the top layer hidden state for attention
    top_hidden = hidden[-1]  # (batch_size, dec_hidden_size)

    # Calculate attention context
    context, attn_weights = self.attention(top_hidden, encoder_outputs, mask)
    # context: (batch_size, enc_hidden_size*2)
    # attn_weights: (batch_size, src_len)

    # Expand context to match embedded dimensions
    context = context.unsqueeze(1)  # (batch_size, 1, enc_hidden_size*2)

    # Concatenate embedded and context
    lstm_input = torch.cat((embedded, context), dim=2)  # (batch_size, 1, embedding_size + enc_hidden_size*2)

    # Pass through LSTM
    output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))
    # output: (batch_size, 1, dec_hidden_size)

    # Concatenate for output layer
    embedded = embedded.squeeze(1)  # (batch_size, embedding_size)
    output = output.squeeze(1)  # (batch_size, dec_hidden_size)
    context = context.squeeze(1)  # (batch_size, enc_hidden_size*2)

    prediction = self.fc_out(torch.cat((output, context, embedded), dim=1))
    # prediction: (batch_size, output_size)

    return prediction, hidden, cell, attn_weights

class Seq2SeqWithAttentionModel(nn.Module):
  def __init__(self, encoder, decoder, device, max_article_len):
    super(Seq2SeqWithAttentionModel, self).__init__()
    self.encoder = encoder
    self.decoder = decoder
    self.device = device
    self.max_article_len = max_article_len

  def create_mask(self, src):
    # Create mask for source sequence
    mask = (src != 0).float()  # 1 for real tokens, 0 for padding
    return mask

  def forward(self, src, tgt, teacher_forcing_ratio=0.5):
    # src: (batch_size, src_len)
    # tgt: (batch_size, tgt_len)

    batch_size = src.shape[0]
    tgt_len = tgt.shape[1]
    tgt_vocab_size = self.decoder.output_size

    # Tensor to store decoder outputs
    outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)

    # Create attention weights storage
    attentions = torch.zeros(batch_size, tgt_len, self.max_article_len).to(self.device)

    # Create mask for src sequence
    mask = self.create_mask(src)

    # Encode the source sequence
    encoder_outputs, hidden, cell = self.encoder(src)

    # First decoder input is the <SOS> token
    decoder_input = tgt[:, 0]

    for t in range(1, tgt_len):
      # Get decoder output for current time step
      output, hidden, cell, attn_weights = self.decoder(
        decoder_input, hidden, cell, encoder_outputs, mask
      )

      # Store output and attention weights
      outputs[:, t, :] = output
      attentions[:, t, :src.shape[1]] = attn_weights

      # Decide whether to use teacher forcing
      teacher_force = random.random() < teacher_forcing_ratio

      # Get the highest predicted token
      top1 = output.argmax(1)

      # Use either target token or predicted token as the next input
      decoder_input = tgt[:, t] if teacher_force else top1

    return outputs, attentions

# Step 4: Training functions with attention weights capture
def train_epoch(model, dataloader, optimizer, criterion, clip=1.0):
  model.train()
  epoch_loss = 0

  for batch_idx, (src, tgt) in enumerate(tqdm(dataloader)):
    src = src.to(device)
    tgt = tgt.to(device)

    optimizer.zero_grad()

    output, _ = model(src, tgt)  # Ignore attention weights during training

    # output: (batch_size, target_len, output_dim)
    # tgt: (batch_size, target_len)

    output_dim = output.shape[-1]

    # Exclude the first token (SOS) in the output calculation
    output = output[:, 1:].reshape(-1, output_dim)
    tgt = tgt[:, 1:].reshape(-1)

    loss = criterion(output, tgt)
    loss.backward()

    # Clip gradients to prevent exploding gradients
    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

    optimizer.step()

    epoch_loss += loss.item()

    # Print progress
    if batch_idx % 100 == 0 and batch_idx > 0:
      print(f"  Batch {batch_idx}/{len(dataloader)} | Loss: {loss.item():.4f}")

  return epoch_loss / len(dataloader)

def evaluate(model, dataloader, criterion):
  model.eval()
  epoch_loss = 0

  with torch.no_grad():
    for src, tgt in dataloader:
      src = src.to(device)
      tgt = tgt.to(device)

      # Feed target as input but with 0 teacher forcing
      output, _ = model(src, tgt, teacher_forcing_ratio=0)  # Ignore attention weights during evaluation

      output_dim = output.shape[-1]

      # Exclude the first token (SOS) in the output calculation
      output = output[:, 1:].reshape(-1, output_dim)
      tgt = tgt[:, 1:].reshape(-1)

      loss = criterion(output, tgt)

      epoch_loss += loss.item()

  return epoch_loss / len(dataloader)

def generate_headline_with_attention(model, article, max_len=20):
  model.eval()

  with torch.no_grad():
    # Convert article to tensor and move to device
    article_tensor = article.unsqueeze(0).to(device)

    # Create mask for src sequence
    mask = model.create_mask(article_tensor)

    # Encode the article
    encoder_outputs, hidden, cell = model.encoder(article_tensor)

    # Start with <SOS> token
    decoder_input = torch.tensor([1]).to(device)  # SOS token index = 1

    headline_indices = [1]  # Start with SOS token
    attention_weights_list = []

    for i in range(max_len):
      # Decode one token at a time
      output, hidden, cell, attn_weights = model.decoder(
        decoder_input, hidden, cell, encoder_outputs, mask
      )

      # Get most probable token
      top_token = output.argmax(1).item()

      # Add token to the headline
      headline_indices.append(top_token)

      # Store attention weights
      attention_weights_list.append(attn_weights.squeeze().cpu().numpy())

      # Stop if EOS token
      if top_token == 2:  # EOS token index = 2
        break

      # Update decoder input
      decoder_input = torch.tensor([top_token]).to(device)

    return headline_indices, attention_weights_list

def calculate_metrics_with_attention(model, test_loader, idx2word):
  model.eval()

  references = []  # Ground truth headlines
  hypotheses = []  # Generated headlines
  attention_data = []  # Store some attention examples

  with torch.no_grad():
    for src, tgt in tqdm(test_loader, desc="Calculating metrics"):
      src = src.to(device)

      for i in range(src.shape[0]):
        # Generate headline for each article in the batch
        predicted_indices, attention_weights = generate_headline_with_attention(model, src[i])

        # Convert to words, removing special tokens
        predicted_tokens = [idx2word.get(idx, '<unk>') for idx in predicted_indices
                           if idx > 3]  # Skip PAD, SOS, EOS, UNK

        # Get reference headline
        reference_indices = tgt[i].tolist()
        reference_tokens = [[idx2word.get(idx, '<unk>') for idx in reference_indices
                           if idx > 3]]  # Skip PAD, SOS, EOS, UNK

        # Add to lists
        hypotheses.append(predicted_tokens)
        references.append(reference_tokens)

        # Store some attention examples (first 5)
        if len(attention_data) < 5:
          article_tokens = [news_data.article_idx2word.get(idx.item(), '<unk>')
                           for idx in src[i] if idx.item() > 0]  # Skip PAD

          attention_data.append({
            'article': article_tokens,
            'headline': predicted_tokens,
            'attention_weights': attention_weights
          })
          # Calculate BLEU-1 to BLEU-4
  bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))
  bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))
  bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))
  bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))

  # Calculate ROUGE
  rouge = Rouge()

  # Format for ROUGE
  hyp_texts = [' '.join(h) for h in hypotheses]
  ref_texts = [' '.join(r[0]) for r in references]

  # If there are empty predictions or references, handle them
  valid_pairs = [(h, r) for h, r in zip(hyp_texts, ref_texts) if h and r]
  if not valid_pairs:
    return {
      'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
      'ROUGE-1': 0, 'ROUGE-2': 0, 'ROUGE-L': 0
    }, attention_data

  hyp_valid, ref_valid = zip(*valid_pairs)

  try:
    rouge_scores = rouge.get_scores(hyp_valid, ref_valid, avg=True)

    return {
      'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
      'ROUGE-1': rouge_scores['rouge-1']['f'],
      'ROUGE-2': rouge_scores['rouge-2']['f'],
      'ROUGE-L': rouge_scores['rouge-l']['f']
    }, attention_data
  except Exception as e:
    print(f"Error calculating ROUGE: {e}")
    return {
      'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
      'ROUGE-1': 0, 'ROUGE-2': 0, 'ROUGE-L': 0
    }, attention_data

# Step 5: Model Initialization and Training
# Model hyperparameters (same as in Task 2 for fair comparison)
input_size = news_data.article_vocab_size
output_size = news_data.headline_vocab_size
embedding_size = 256
hidden_size = 512
num_layers = 2
dropout = 0.5

# Initialize encoder and decoder with attention
encoder = EncoderLSTM(input_size, embedding_size, hidden_size, num_layers, dropout)
decoder = AttentionDecoderLSTM(output_size, embedding_size, hidden_size, hidden_size, num_layers, dropout)

# Create the Seq2Seq model with attention
model = Seq2SeqWithAttentionModel(encoder, decoder, device, news_data.max_article_len).to(device)

# Print model summary
print(model)

import random

# Set up the loss function and optimizer
pad_idx = 0  # PAD token index
criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)
optimizer = optim.Adam(model.parameters())

# Training parameters
n_epochs = 10
clip = 1.0
best_valid_loss = float('inf')

# Lists to store losses
train_losses = []
val_losses = []

# To store training time
start_time = time.time()

# Training loop
print("Starting training...")
for epoch in range(n_epochs):
  # Train
  train_loss = train_epoch(model, train_loader, optimizer, criterion, clip)
  train_losses.append(train_loss)

  # Evaluate
  val_loss = evaluate(model, val_loader, criterion)
  val_losses.append(val_loss)

  # Save best model
  if val_loss < best_valid_loss:
    best_valid_loss = val_loss
    torch.save(model.state_dict(), 'headline_generator_with_attention.pt')
    print(f"Model saved at epoch {epoch+1}")

  # Print progress
  print(f"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

# Calculate training time
training_time = time.time() - start_time
print(f"Training completed in {training_time:.2f} seconds")

# Calculate training time
training_time = time.time() - start_time
print(f"Training completed in {training_time:.2f} seconds")

# Step 6: Evaluation
# Load the best model
model.load_state_dict(torch.load('headline_generator_with_attention.pt'))

# Calculate inference time and metrics
inference_start_time = time.time()
metrics, attention_examples = calculate_metrics_with_attention(model, test_loader, news_data.headline_idx2word)

# Print metrics
print("\nEvaluation Metrics with Attention:")
for metric, value in metrics.items():
    print(f"{metric}: {value:.4f}")

# Calculate inference time
inference_time = time.time() - inference_start_time
print(f"Inference completed in {inference_time:.2f} seconds")

# Step 7: Visualization of Attention Weights
def visualize_attention(article_tokens, headline_tokens, attention_weights, example_idx=0):
    """
    Visualize attention weights between article tokens and generated headline tokens
    """
    # Filter out padding tokens
    article_tokens = [token for token in article_tokens if token != '<pad>']
    headline_tokens = [token for token in headline_tokens if token not in ['<pad>', '<sos>']]

    # Trim attention weights to match article length
    attention_weights = [weights[:len(article_tokens)] for weights in attention_weights]

    # Create figure
    fig, ax = plt.subplots(figsize=(14, 10))

    # Display attention heatmap
    cax = ax.matshow(attention_weights, cmap='viridis')

    # Add colorbar
    fig.colorbar(cax)

    # Set axis labels
    ax.set_xticklabels([''] + article_tokens, rotation=90)
    ax.set_yticklabels([''] + headline_tokens)

    # Show tick marks
    ax.xaxis.set_major_locator(plt.MultipleLocator(1))
    ax.yaxis.set_major_locator(plt.MultipleLocator(1))

    # Set labels
    plt.xlabel('Article Tokens')
    plt.ylabel('Generated Headline Tokens')
    plt.title(f'Attention Visualization (Example {example_idx+1})')

    plt.tight_layout()
    return fig

# Visualize attention for examples
print("\nVisualizing attention weights for sample examples...")
for i, example in enumerate(attention_examples):
    article_tokens = example['article']
    headline_tokens = example['headline']
    attention_weights = example['attention_weights']

    # Limit visualization to first 50 tokens of article for clarity
    if len(article_tokens) > 50:
        article_tokens = article_tokens[:50]
        attention_weights = [w[:50] for w in attention_weights]

    # Print the example
    print(f"\nExample {i+1}:")
    print(f"Article (truncated): {' '.join(article_tokens)}")
    print(f"Generated Headline: {' '.join(headline_tokens)}")

    # Create visualization
    fig = visualize_attention(article_tokens, headline_tokens, attention_weights, i)
    plt.savefig(f'attention_example_{i+1}.png')
    plt.close(fig)

    print(f"Attention visualization saved as attention_example_{i+1}.png")

# Step 8: Generate Sample Headlines with Attention Visualization
def generate_and_visualize_samples(model, test_loader, idx2word, num_samples=5):
    """Generate sample headlines and visualize their attention weights"""
    model.eval()
    samples = []

    with torch.no_grad():
        for src, tgt in test_loader:
            src = src.to(device)
            tgt = tgt.to(device)

            for i in range(min(num_samples, src.shape[0])):
                # Generate headline
                predicted_indices, attention_weights = generate_headline_with_attention(model, src[i])

                # Convert indices to tokens
                article_tokens = [news_data.article_idx2word.get(idx.item(), '<unk>')
                                 for idx in src[i] if idx.item() > 0]  # Skip PAD

                predicted_tokens = [idx2word.get(idx, '<unk>') for idx in predicted_indices
                                   if idx not in [0, 1, 2]]  # Skip PAD, SOS, EOS

                reference_tokens = [idx2word.get(idx.item(), '<unk>') for idx in tgt[i]
                                   if idx.item() not in [0, 1, 2]]  # Skip PAD, SOS, EOS

                # Print comparison
                print(f"\nSample {len(samples) + 1}:")
                print(f"Article (truncated): {' '.join(article_tokens[:50])}...")
                print(f"Reference Headline: {' '.join(reference_tokens)}")
                print(f"Generated Headline: {' '.join(predicted_tokens)}")

                # Store sample
                samples.append({
                    'article': article_tokens,
                    'reference': reference_tokens,
                    'generated': predicted_tokens,
                    'attention_weights': attention_weights
                })

                # Visualize attention
                if len(article_tokens) > 50:
                    vis_tokens = article_tokens[:50]
                    vis_weights = [w[:50] for w in attention_weights]
                else:
                    vis_tokens = article_tokens
                    vis_weights = attention_weights

                fig = visualize_attention(vis_tokens, predicted_tokens, vis_weights, len(samples))
                plt.savefig(f'sample_attention_{len(samples)}.png')
                plt.close(fig)

            if len(samples) >= num_samples:
                break

    return samples

# Generate and visualize sample headlines
print("\nGenerating sample headlines with attention visualization...")
samples = generate_and_visualize_samples(model, test_loader, news_data.headline_idx2word)

# Step 9: Compare with Non-Attention Model (Assuming you have results from Task 2)
# Note: You'll need to have the metrics from Task 2 saved or load them from a file
# For demonstration, let's create a function to load/simulate previous results

def load_previous_model_results():
    """
    Load or simulate results from the previous non-attention model (Task 2)
    In a real scenario, you would load actual saved metrics from Task 2
    """
    # These would be the actual metrics from Task 2
    # Placeholder values for demonstration
    return {
        'BLEU-1': 0.0000,
        'BLEU-2': 0.0000,
        'BLEU-3': 0.0000,
        'BLEU-4': 0.0000,
        'ROUGE-1': 0.0000,
        'ROUGE-2': 0.0000,
        'ROUGE-L': 0.0000,
        'Training Time': 340.52,  # seconds
        'Inference Time': 9.29   # seconds
    }

# Load previous model results
previous_metrics = load_previous_model_results()

# Compare current model with previous model
print("\nPerformance Comparison:")
print("-------------------------------------------------------")
print("Metric          | Without Attention | With Attention   | Improvement")
print("-------------------------------------------------------")
for metric in ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']:
    prev_value = previous_metrics.get(metric, 0)
    curr_value = metrics.get(metric, 0)
    improvement = ((curr_value - prev_value) / prev_value) * 100 if prev_value > 0 else float('inf')

    print(f"{metric:15} | {prev_value:.4f}           | {curr_value:.4f}         | {improvement:+.2f}%")

print("-------------------------------------------------------")
print(f"Training Time   | {previous_metrics.get('Training Time', 0):.2f}s          | {training_time:.2f}s        | {((training_time - previous_metrics.get('Training Time', 0)) / previous_metrics.get('Training Time', 0)) * 100:+.2f}%")
print(f"Inference Time  | {previous_metrics.get('Inference Time', 0):.2f}s          | {inference_time:.2f}s        | {((inference_time - previous_metrics.get('Inference Time', 0)) / previous_metrics.get('Inference Time', 0)) * 100:+.2f}%")
print("-------------------------------------------------------")

# Step 10: Plot Attention Heatmaps for Analysis
def plot_full_attention_matrix(example_idx=0):
    """
    Create a full attention heatmap for a single example
    """
    example = attention_examples[example_idx]
    article_tokens = example['article']
    headline_tokens = example['headline']
    attention_weights = example['attention_weights']

    # Limit to a reasonable size for visualization
    max_article_tokens = 100
    if len(article_tokens) > max_article_tokens:
        article_tokens = article_tokens[:max_article_tokens]
        attention_weights = [w[:max_article_tokens] for w in attention_weights]

    # Create attention matrix
    attn_matrix = np.array(attention_weights)

    # Create figure
    plt.figure(figsize=(20, 10))

    # Plot heatmap
    sns.heatmap(attn_matrix, xticklabels=article_tokens, yticklabels=headline_tokens,
                cmap='viridis', cbar=True)

    plt.title(f'Full Attention Matrix for Example {example_idx+1}')
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()

    plt.savefig(f'full_attention_matrix_{example_idx+1}.png')
    plt.close()

    print(f"Full attention matrix saved as full_attention_matrix_{example_idx+1}.png")

# Try to import seaborn for better visualizations
try:
    import seaborn as sns
    print("\nCreating detailed attention heatmaps...")
    for i in range(min(3, len(attention_examples))):
        plot_full_attention_matrix(i)
except ImportError:
    print("\nSeaborn not available for detailed heatmaps. Install with: pip install seaborn")

# Step 11: Analysis of Attention Patterns
def analyze_attention_patterns(attention_examples):
    """
    Analyze the patterns in attention weights
    """
    print("\nAnalyzing attention patterns...")

    # Calculate average attention per position
    position_attention = []
    for example in attention_examples:
        weights = example['attention_weights']
        for i, token_weights in enumerate(weights):
            # Ensure position_attention has enough elements
            while len(position_attention) <= i:
                position_attention.append([])
            position_attention[i].extend(token_weights)

    # Calculate averages
    avg_position_attention = [sum(pos) / len(pos) if pos else 0 for pos in position_attention]

    # Plot average attention by position
    plt.figure(figsize=(12, 6))
    plt.plot(avg_position_attention[:50])  # Limit to first 50 positions
    plt.title('Average Attention by Article Token Position')
    plt.xlabel('Token Position in Article')
    plt.ylabel('Average Attention Weight')
    plt.grid(True)
    plt.savefig('avg_attention_by_position.png')
    plt.close()

    print("Average attention by position saved as avg_attention_by_position.png")

    # Check which tokens receive most attention
    token_attention = {}
    for example in attention_examples:
        article_tokens = example['article']
        weights = example['attention_weights']

        for token_weights in weights:
            # Find top attended tokens
            for i, weight in enumerate(token_weights):
                if i < len(article_tokens):
                    token = article_tokens[i]
                    if token not in token_attention:
                        token_attention[token] = []
                    token_attention[token].append(weight)

    # Calculate average attention per token
    avg_token_attention = {token: sum(weights) / len(weights)
                         for token, weights in token_attention.items()}

    # Get top attended tokens
    top_tokens = sorted(avg_token_attention.items(), key=lambda x: x[1], reverse=True)[:20]

    print("\nTop 20 tokens by average attention weight:")
    for token, weight in top_tokens:
        print(f"{token}: {weight:.4f}")

    # Visualize top tokens
    plt.figure(figsize=(14, 8))
    tokens, weights = zip(*top_tokens)
    plt.bar(tokens, weights)
    plt.title('Top 20 Tokens by Average Attention Weight')
    plt.xlabel('Token')
    plt.ylabel('Average Attention Weight')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig('top_attention_tokens.png')
    plt.close()

    print("Top attention tokens visualization saved as top_attention_tokens.png")

# Run attention pattern analysis
analyze_attention_patterns(attention_examples)

# Step 12: Save Final Report and Model
def save_final_report(metrics, training_time, inference_time):
    """
    Save a comprehensive report of the model's performance
    """
    with open('headline_generation_with_attention_report.txt', 'w') as f:
        f.write("HEADLINE GENERATION WITH ATTENTION - FINAL REPORT\n")
        f.write("=================================================\n\n")

        f.write("MODEL ARCHITECTURE:\n")
        f.write("-----------------\n")
        f.write("Encoder: Bidirectional LSTM with embedding\n")
        f.write(f"Encoder Hidden Size: {hidden_size}\n")
        f.write(f"Decoder: LSTM with attention mechanism\n")
        f.write(f"Decoder Hidden Size: {hidden_size}\n")
        f.write(f"Embedding Size: {embedding_size}\n")
        f.write(f"Number of Layers: {num_layers}\n")
        f.write(f"Dropout: {dropout}\n\n")

        f.write("TRAINING DETAILS:\n")
        f.write("----------------\n")
        f.write(f"Number of Epochs: {n_epochs}\n")
        f.write(f"Batch Size: {batch_size}\n")
        f.write(f"Optimizer: Adam\n")
        f.write(f"Loss Function: CrossEntropyLoss\n")
        f.write(f"Training Time: {training_time:.2f} seconds\n\n")

        f.write("EVALUATION METRICS:\n")
        f.write("-----------------\n")
        for metric, value in metrics.items():
            f.write(f"{metric}: {value:.4f}\n")
        f.write(f"Inference Time: {inference_time:.2f} seconds\n\n")

        f.write("CONCLUSIONS:\n")
        f.write("-----------\n")
        f.write("The attention mechanism allows the model to focus on relevant parts of the\n")
        f.write("input article when generating each word of the headline. This provides several benefits:\n\n")

        f.write("1. Improved performance on metrics like BLEU and ROUGE\n")
        f.write("2. Better handling of long input sequences\n")
        f.write("3. Interpretability through attention visualizations\n")
        f.write("4. More coherent and contextually relevant headlines\n\n")

        f.write("The attention visualizations show that the model tends to focus on specific\n")
        f.write("informative words in the article that are relevant for headline generation.\n")
        f.write("This demonstrates that the model is learning to identify the most important\n")
        f.write("parts of the article for summarization purposes.\n")

# Save final report
save_final_report(metrics, training_time, inference_time)
print("\nFinal report saved as headline_generation_with_attention_report.txt")

# Save model and config for future use
torch.save({
    'model_state_dict': model.state_dict(),
    'encoder_state_dict': encoder.state_dict(),
    'decoder_state_dict': decoder.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'article_vocab_size': input_size,
    'headline_vocab_size': output_size,
    'embedding_size': embedding_size,
    'hidden_size': hidden_size,
    'num_layers': num_layers,
    'dropout': dropout,
    'article_word2idx': news_data.article_word2idx,
    'headline_word2idx': news_data.headline_word2idx,
    'article_idx2word': news_data.article_idx2word,
    'headline_idx2word': news_data.headline_idx2word,
}, 'headline_generator_with_attention_full.pt')

print("Model and configuration saved as headline_generator_with_attention_full.pt")
print("\nTask 3 completed successfully!")

# Optional: Plot training and validation loss curves
plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.savefig('loss_curves.png')
plt.show()

print("Training and validation loss curves saved as loss_curves.png")

"""## Task 4: Implement Encoder-Decoder with Self-Attention (Transformer)
## Based on the paper: "Attention Is All You Need" by Vaswani et al.
"""

# Import required libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
import math
import time
from tqdm.notebook import tqdm
import copy

# Using the same dataset and processing from previous tasks
# We'll reuse the NewsDataset and HeadlineDataset classes

# Step 1: Implement Transformer Components

class PositionalEncoding(nn.Module):
    """
    Positional encoding for the transformer model.
    Adds positional information to the input embeddings.
    """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create positional encoding matrix
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # Apply sine to even indices and cosine to odd indices
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # Add batch dimension and register as buffer (not a parameter)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: Tensor, shape [batch_size, seq_len, embedding_dim]
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class MultiHeadAttention(nn.Module):
    """
    Multi-head attention mechanism that allows the model to attend to
    information from different representation subspaces.
    """
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()

        # Ensure d_model is divisible by num_heads
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # Linear projections for Q, K, V, and output
        self.wq = nn.Linear(d_model, d_model)
        self.wk = nn.Linear(d_model, d_model)
        self.wv = nn.Linear(d_model, d_model)
        self.wo = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x):
        """
        Split the last dimension into (num_heads, d_k)
        and transpose to (batch_size, num_heads, seq_len, d_k)
        """
        batch_size, seq_len, _ = x.size()
        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

    def merge_heads(self, x):
        """
        Merge heads back together
        """
        batch_size, _, seq_len, _ = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)

    def scaled_dot_product_attention(self, q, k, v, mask=None):
        """
        Compute scaled dot-product attention
        """
        # Calculate attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)

        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Apply softmax and get attention weights
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Apply attention weights to values
        output = torch.matmul(attn_weights, v)

        return output, attn_weights

    def forward(self, query, key, value, mask=None):
        """
        Args:
            query, key, value: Tensors of shape [batch_size, seq_len, d_model]
            mask: Optional mask tensor
        """
        batch_size = query.size(0)

        # Linear projections and split heads
        q = self.split_heads(self.wq(query))
        k = self.split_heads(self.wk(key))
        v = self.split_heads(self.wv(value))

        # Apply scaled dot-product attention
        attn_output, attn_weights = self.scaled_dot_product_attention(q, k, v, mask)

        # Merge heads and apply final linear projection
        output = self.wo(self.merge_heads(attn_output))

        return output, attn_weights

class PositionwiseFeedForward(nn.Module):
    """
    Position-wise feed-forward network as described in the paper
    """
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.fc2(self.dropout(F.relu(self.fc1(x))))

class EncoderLayer(nn.Module):
    """
    Single layer of the transformer encoder
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(EncoderLayer, self).__init__()

        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        Args:
            x: Input tensor
            mask: Optional mask tensor
        """
        # Self-attention with residual connection and layer norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # Feed-forward with residual connection and layer norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

class DecoderLayer(nn.Module):
    """
    Single layer of the transformer decoder
    """
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(DecoderLayer, self).__init__()

        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):
        """
        Args:
            x: Decoder input tensor
            enc_output: Output from encoder
            src_mask: Mask for source sequence
            tgt_mask: Mask for target sequence (to prevent attending to future tokens)
        """
        # Self-attention with residual connection and layer norm
        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(self_attn_output))

        # Cross-attention with residual connection and layer norm
        cross_attn_output, cross_attn_weights = self.cross_attn(x, enc_output, enc_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn_output))

        # Feed-forward with residual connection and layer norm
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x, cross_attn_weights

class TransformerEncoder(nn.Module):
    """
    Full transformer encoder stack
    """
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, max_seq_len=5000):
        super(TransformerEncoder, self).__init__()

        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)

        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(d_model)

    def forward(self, src, src_mask=None):
        """
        Args:
            src: Source token indices [batch_size, src_len]
            src_mask: Mask for source sequence [batch_size, 1, src_len]
        """
        # Convert token indices to embeddings and add positional encoding
        x = self.embedding(src) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)

        # Apply encoder layers
        for layer in self.layers:
            x = layer(x, src_mask)

        # Apply final normalization
        x = self.norm(x)

        return x

class TransformerDecoder(nn.Module):
    """
    Full transformer decoder stack
    """
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, max_seq_len=5000):
        super(TransformerDecoder, self).__init__()

        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)

        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(d_model)
        self.fc_out = nn.Linear(d_model, vocab_size)

    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):
        """
        Args:
            tgt: Target token indices [batch_size, tgt_len]
            enc_output: Output from encoder [batch_size, src_len, d_model]
            src_mask: Mask for source sequence
            tgt_mask: Mask for target sequence (prevents attending to future tokens)
        """
        # Convert token indices to embeddings and add positional encoding
        x = self.embedding(tgt) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)

        # Store attention weights from all layers
        all_attn_weights = []

        # Apply decoder layers
        for layer in self.layers:
            x, attn_weights = layer(x, enc_output, src_mask, tgt_mask)
            all_attn_weights.append(attn_weights)

        # Apply final normalization
        x = self.norm(x)

        # Project to vocabulary
        output = self.fc_out(x)

        return output, all_attn_weights

class Transformer(nn.Module):
    """
    Full transformer model for sequence-to-sequence tasks
    """
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,
                 d_ff=2048, num_layers=6, dropout=0.1, max_seq_len=5000):
        super(Transformer, self).__init__()

        self.encoder = TransformerEncoder(
            src_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_seq_len)

        self.decoder = TransformerDecoder(
            tgt_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_seq_len)

        self.pad_idx = 0  # Assuming PAD token index is 0

    def create_src_mask(self, src):
        """
        Create mask for source sequence (to ignore padding tokens)
        """
        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)
        return src_mask

    def create_tgt_mask(self, tgt):
        """
        Create mask for target sequence (to prevent attending to future tokens and padding)
        """
        tgt_len = tgt.size(1)

        # Create padding mask
        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(3)

        # Create causal (future) mask
        tgt_future_mask = torch.triu(
            torch.ones((tgt_len, tgt_len), device=tgt.device), diagonal=1).bool()
        tgt_future_mask = ~tgt_future_mask.unsqueeze(0).unsqueeze(1)

        # Combine masks
        tgt_mask = tgt_pad_mask & tgt_future_mask

        return tgt_mask

    def forward(self, src, tgt):
        """
        Args:
            src: Source token indices [batch_size, src_len]
            tgt: Target token indices [batch_size, tgt_len]
        """
        # Create masks
        src_mask = self.create_src_mask(src)
        tgt_mask = self.create_tgt_mask(tgt)

        # Encode source sequence
        enc_output = self.encoder(src, src_mask)

        # Decode target sequence
        output, attention_weights = self.decoder(tgt, enc_output, src_mask, tgt_mask)

        return output, attention_weights

# Step 2: Training and Evaluation Functions

def train_epoch_transformer(model, dataloader, optimizer, criterion, clip=1.0):
    model.train()
    epoch_loss = 0

    for batch_idx, (src, tgt) in enumerate(tqdm(dataloader)):
        src = src.to(device)
        tgt = tgt.to(device)

        # Target for input: all tokens except the last one
        tgt_input = tgt[:, :-1]
        # Target for output: all tokens except the first one (SOS)
        tgt_output = tgt[:, 1:]

        optimizer.zero_grad()

        # Forward pass
        output, _ = model(src, tgt_input)

        # Reshape output and target for loss calculation
        output_dim = output.shape[-1]
        output = output.reshape(-1, output_dim)
        tgt_output = tgt_output.reshape(-1)

        # Calculate loss
        loss = criterion(output, tgt_output)
        loss.backward()

        # Clip gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()

        # Print progress
        if batch_idx % 100 == 0 and batch_idx > 0:
            print(f"  Batch {batch_idx}/{len(dataloader)} | Loss: {loss.item():.4f}")

    return epoch_loss / len(dataloader)

def evaluate_transformer(model, dataloader, criterion):
    model.eval()
    epoch_loss = 0

    with torch.no_grad():
        for src, tgt in dataloader:
            src = src.to(device)
            tgt = tgt.to(device)

            # Target for input: all tokens except the last one
            tgt_input = tgt[:, :-1]
            # Target for output: all tokens except the first one (SOS)
            tgt_output = tgt[:, 1:]

            # Forward pass
            output, _ = model(src, tgt_input)

            # Reshape output and target for loss calculation
            output_dim = output.shape[-1]
            output = output.reshape(-1, output_dim)
            tgt_output = tgt_output.reshape(-1)

            # Calculate loss
            loss = criterion(output, tgt_output)

            epoch_loss += loss.item()

    return epoch_loss / len(dataloader)

def generate_headline_transformer(model, article, max_len=20):
    model.eval()

    with torch.no_grad():
        # Convert article to tensor and move to device
        article_tensor = article.unsqueeze(0).to(device)

        # Create source mask
        src_mask = model.create_src_mask(article_tensor)

        # Encode the article
        enc_output = model.encoder(article_tensor, src_mask)

        # Start with < SOS > token
        headline_indices = [1]  # SOS token index = 1
        all_attention_weights = []

        for i in range(max_len):
            # Convert current headline to tensor
            tgt_tensor = torch.LongTensor([headline_indices]).to(device)

            # Create target mask
            tgt_mask = model.create_tgt_mask(tgt_tensor)

            # Decode
            output, attention_weights = model.decoder(tgt_tensor, enc_output, src_mask, tgt_mask)

            # Get most probable token from the last position
            pred_token = output[:, -1, :].argmax(dim=1).item()

            # Add token to the headline
            headline_indices.append(pred_token)

            # Store attention weights (from the last decoder layer)
            all_attention_weights.append(attention_weights[-1][:, -1, :].cpu().numpy())

            # Stop if EOS token
            if pred_token == 2:  # EOS token index = 2
                break

        return headline_indices, all_attention_weights

def calculate_metrics_transformer(model, test_loader, idx2word):
    model.eval()

    references = []  # Ground truth headlines
    hypotheses = []  # Generated headlines
    attention_data = []  # Store some attention examples

    with torch.no_grad():
        for src, tgt in tqdm(test_loader, desc="Calculating metrics"):
            src = src.to(device)

            for i in range(src.shape[0]):
                # Generate headline for each article in the batch
                predicted_indices, attention_weights = generate_headline_transformer(model, src[i])

                # Convert to words, removing special tokens
                predicted_tokens = [idx2word.get(idx, '<unk>') for idx in predicted_indices
                                   if idx > 3]  # Skip PAD, SOS, EOS, UNK

                # Get reference headline
                reference_indices = tgt[i].tolist()
                reference_tokens = [[idx2word.get(idx, '<unk>') for idx in reference_indices
                                   if idx > 3]]  # Skip PAD, SOS, EOS, UNK

                # Add to lists
                hypotheses.append(predicted_tokens)
                references.append(reference_tokens)

                # Store some attention examples (first 5)
                if len(attention_data) < 5:
                    article_tokens = [news_data.article_idx2word.get(idx.item(), '<unk>')
                                     for idx in src[i] if idx.item() > 0]  # Skip PAD

                    attention_data.append({
                        'article': article_tokens,
                        'headline': predicted_tokens,
                        'attention_weights': attention_weights
                    })

    # Calculate BLEU-1 to BLEU-4
    bleu1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))
    bleu2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))
    bleu3 = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0))
    bleu4 = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))

    # Calculate ROUGE
    rouge = Rouge()

    # Format for ROUGE
    hyp_texts = [' '.join(h) for h in hypotheses]
    ref_texts = [' '.join(r[0]) for r in references]

    # If there are empty predictions or references, handle them
    valid_pairs = [(h, r) for h, r in zip(hyp_texts, ref_texts) if h and r]
    if not valid_pairs:
        return {
            'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
            'ROUGE-1': 0, 'ROUGE-2': 0, 'ROUGE-L': 0
        }, attention_data

    hyp_valid, ref_valid = zip(*valid_pairs)

    try:
        rouge_scores = rouge.get_scores(hyp_valid, ref_valid, avg=True)

        return {
            'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
            'ROUGE-1': rouge_scores['rouge-1']['f'],
            'ROUGE-2': rouge_scores['rouge-2']['f'],
            'ROUGE-L': rouge_scores['rouge-l']['f']
        }, attention_data
    except Exception as e:
        print(f"Error calculating ROUGE: {e}")
        return {
            'BLEU-1': bleu1, 'BLEU-2': bleu2, 'BLEU-3': bleu3, 'BLEU-4': bleu4,
            'ROUGE-1': 0, 'ROUGE-2': 0, 'ROUGE-L': 0
        }, attention_data

# Visualization function for transformer attention
def visualize_transformer_attention(article_tokens, headline_tokens, attention_weights, example_idx=0):
    """
    Visualize attention weights between article tokens and generated headline tokens
    for the transformer model
    """
    # Filter out padding tokens
    article_tokens = [token for token in article_tokens if token != '<pad>']
    headline_tokens = [token for token in headline_tokens if token not in ['<pad>', '<sos>']]

    # Trim attention weights to match article length
    attention_weights = [weights[:len(article_tokens)] for weights in attention_weights]

    # Create figure
    fig, ax = plt.subplots(figsize=(14, 10))

    # Display attention heatmap
    cax = ax.matshow(attention_weights, cmap='viridis')

    # Add colorbar
    fig.colorbar(cax)

    # Set axis labels
    ax.set_xticklabels([''] + article_tokens, rotation=90)
    ax.set_yticklabels([''] + headline_tokens)

    # Show tick marks
    ax.xaxis.set_major_locator(plt.MultipleLocator(1))
    ax.yaxis.set_major_locator(plt.MultipleLocator(1))

    # Set labels
    plt.xlabel('Article Tokens')
    plt.ylabel('Generated Headline Tokens')
    plt.title(f'Transformer Attention Visualization (Example {example_idx+1})')

    plt.tight_layout()
    return fig

# Step 3: Model Initialization and Training

# Model hyperparameters
src_vocab_size = news_data.article_vocab_size
tgt_vocab_size = news_data.headline_vocab_size
d_model = 256  # Embedding dimension
num_heads = 8  # Number of attention heads
d_ff = 1024  # Feed-forward dimension
num_layers = 4  # Number of encoder/decoder layers
dropout = 0.1  # Dropout rate

# Initialize transformer model
transformer = Transformer(
    src_vocab_size=src_vocab_size,
    tgt_vocab_size=tgt_vocab_size,
    d_model=d_model,
    num_heads=num_heads,
    d_ff=d_ff,
    num_layers=num_layers,
    dropout=dropout
).to(device)

# Print model summary
print(transformer)
print(f"Number of parameters: {sum(p.numel() for p in transformer.parameters() if p.requires_grad)}")

# Set up loss function and optimizer
criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index (0)
optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

# Create learning rate scheduler
warmup_steps = 4000
scheduler = optim.lr_scheduler.LambdaLR(optimizer,lambda step: d_model**(-0.5) * min((step + 1)**(-0.5), (step + 1) * warmup_steps**(-1.5))) # Add

# Training parameters
n_epochs = 20
clip = 1.0
best_valid_loss = float('inf')

# Lists to store losses
train_losses = []
val_losses = []

# To store training time
start_time = time.time()

# Training loop
print("Starting transformer training...")
for epoch in range(n_epochs):
    # Train
    train_loss = train_epoch_transformer(transformer, train_loader, optimizer, criterion, clip)
    train_losses.append(train_loss)

    # Evaluate
    val_loss = evaluate_transformer(transformer, val_loader, criterion)
    val_losses.append(val_loss)

    # Update learning rate
    scheduler.step()

    # Save best model
    if val_loss < best_valid_loss:
        best_valid_loss = val_loss
        torch.save(transformer.state_dict(), 'headline_transformer.pt')
        print(f"Model saved at epoch {epoch+1}")

    # Print progress
    print(f"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

# Calculate training time
training_time = time.time() - start_time
print(f"Training completed in {training_time:.2f} seconds")

# Step 4: Evaluation
# Load the best model
transformer.load_state_dict(torch.load('headline_transformer.pt'))

# Calculate inference time and metrics
inference_start_time = time.time()
metrics, attention_examples = calculate_metrics_transformer(transformer, test_loader, news_data.headline_idx2word)

# Print metrics
print("\nEvaluation Metrics with Transformer:")
for metric, value in metrics.items():
    print(f"{metric}: {value:.4f}")

# Calculate inference time
inference_time = time.time() - inference_start_time
print(f"Inference completed in {inference_time:.2f} seconds")

def visualize_transformer_attention(article_tokens, headline_tokens, attention_weights, example_idx):
    """Visualize attention weights between article and headline tokens."""

    # Filter out special tokens from the tokens
    article_tokens = [token for token in article_tokens if token not in ['<pad>', '<sos>', '<eos>']]
    headline_tokens = [token for token in headline_tokens if token not in ['<pad>', '<sos>', '<eos>']]

    # Get the maximum article length to pad shorter attention weight vectors
    max_article_len = max(len(w) for w in attention_weights)

    # Pad attention weights for each decoder timestep to the maximum length
    padded_attention_weights = [np.pad(w, (0, max_article_len - len(w)), 'constant')
                               for w in attention_weights]

    # Create the attention heatmap visualization
    fig, ax = plt.subplots(figsize=(14, 10))
    cax = ax.matshow(padded_attention_weights, cmap='viridis')  # Using padded weights
    fig.colorbar(cax)

    # Set axis labels
    ax.set_xticklabels([''] + article_tokens, rotation=90)
    ax.set_yticklabels([''] + headline_tokens)

    # Show title
    ax.set_title(f"Transformer Attention Weights - Example {example_idx + 1}")

    return fig  # Return the figure

# Step 5: Visualization and Analysis
# Visualize attention for examples
print("\nVisualizing transformer attention weights for sample examples...")
for i, example in enumerate(attention_examples):
    article_tokens = example['article']
    headline_tokens = example['headline']
    attention_weights = example['attention_weights']

    # Limit visualization to first 50 tokens of article for clarity
    if len(article_tokens) > 50:
        article_tokens = article_tokens[:50]
        attention_weights = [w[:50] for w in attention_weights]

    # Print the example
    print(f"\nExample {i+1}:")
    print(f"Article (truncated): {' '.join(article_tokens)}")
    print(f"Generated Headline: {' '.join(headline_tokens)}")

def generate_and_visualize_transformer_samples(model, test_loader, idx2word, num_samples=5):
    """Generate sample headlines with transformer and visualize attention"""
    model.eval()
    samples = []

    with torch.no_grad():
        for src, tgt in test_loader:
            src = src.to(device)
            tgt = tgt.to(device)

            for i in range(min(num_samples, src.shape[0])):
                # Generate headline
                predicted_indices, attention_weights = generate_headline_transformer(model, src[i])

                # Convert indices to tokens
                article_tokens = [news_data.article_idx2word.get(idx.item(), '<unk>')
                                 for idx in src[i] if idx.item() > 0]  # Skip PAD

                predicted_tokens = [idx2word.get(idx, '<unk>') for idx in predicted_indices
                                   if idx not in [0, 1, 2]]  # Skip PAD, SOS, EOS

                reference_tokens = [idx2word.get(idx.item(), '<unk>') for idx in tgt[i]
                                   if idx.item() not in [0, 1, 2]]  # Skip PAD, SOS, EOS

                # Print comparison
                print(f"\nSample {len(samples) + 1}:")
                print(f"Article (truncated): {' '.join(article_tokens[:50])}...")
                print(f"Reference Headline: {' '.join(reference_tokens)}")
                print(f"Generated Headline: {' '.join(predicted_tokens)}")

                # Visualize attention weights
                fig = visualize_transformer_attention(
                    article_tokens[:50],  # Truncate for clarity
                    predicted_tokens,
                    [weights[:50] for weights in attention_weights],  # Truncate attention weights
                    len(samples)
                )
                plt.savefig(f'transformer_sample_{len(samples) + 1}.png')
                plt.close(fig)

                samples.append({
                    'article': article_tokens,
                    'reference': reference_tokens,
                    'prediction': predicted_tokens,
                    'attention': attention_weights
                })

                if len(samples) >= num_samples:
                    return samples

            if len(samples) >= num_samples:
                break

    return samples

seq2seq_metrics = {
    'BLEU-1': 0.0000,
    'BLEU-4': 0.0000,
    'ROUGE-L': 0.0000
}
seq2seq_training_time = 340.52  # in seconds
seq2seq_params = 13384190

attention_metrics = {
    'BLEU-1': 0.0484,
    'BLEU-4': 0.0153,
    'ROUGE-L': 0.1496
}
attention_training_time = 772.70  # in seconds
attention_params = 27851614

# Step 7: Compare with previous models
print("\nComparison of all models:")
print("====================================")
print("Model | BLEU-1 | BLEU-4 | ROUGE-L | Training Time (s) | Parameters")
print("====================================")
print(f"Seq2Seq | {seq2seq_metrics['BLEU-1']:.4f} | {seq2seq_metrics['BLEU-4']:.4f} | {seq2seq_metrics['ROUGE-L']:.4f} | {seq2seq_training_time:.2f} | {seq2seq_params}")
print(f"Seq2Seq+Attention | {attention_metrics['BLEU-1']:.4f} | {attention_metrics['BLEU-4']:.4f} | {attention_metrics['ROUGE-L']:.4f} | {attention_training_time:.2f} | {attention_params}")
print(f"Transformer | {metrics['BLEU-1']:.4f} | {metrics['BLEU-4']:.4f} | {metrics['ROUGE-L']:.4f} | {training_time:.2f} | {sum(p.numel() for p in transformer.parameters() if p.requires_grad)}")
print("====================================")

# Step 8: Plot training and validation loss
plt.figure(figsize=(12, 6))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Transformer Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.savefig('transformer_loss.png')
plt.show()

# Step 9: Analysis and Discussion

def analyze_attention_patterns(attention_examples):
    """Analyze patterns in attention weights with robust error handling"""
    print("\nAnalyzing attention patterns:")

    for i, example in enumerate(attention_examples[:3]):  # Analyze first 3 examples
        article_tokens = example['article']
        headline_tokens = example['headline']
        attention_weights = example['attention_weights']

        print(f"\nExample {i+1} attention analysis:")
        print(f"  Headline: {' '.join(headline_tokens)}")
        print(f"  Article excerpt: {' '.join(article_tokens[:20])}...")

        # Print the shape and type of attention weights for debugging
        print(f"  Attention weights type: {type(attention_weights)}")
        if isinstance(attention_weights, list):
            print(f"  Attention weights list length: {len(attention_weights)}")
            if len(attention_weights) > 0:
                first_weight = attention_weights[0]
                print(f"  First weight type: {type(first_weight)}")
                if hasattr(first_weight, 'shape'):
                    print(f"  First weight shape: {first_weight.shape}")
        elif hasattr(attention_weights, 'shape'):
            print(f"  Attention weights shape: {attention_weights.shape}")

        # Simple analysis without attempting complex indexing
        print("  Due to the complex structure of transformer attention weights,")
        print("  detailed token-level analysis is skipped in this version.")
        print("  Consider visualizing attention using the provided visualization functions instead.")

# Run the safer analysis function
try:
    analyze_attention_patterns(attention_examples)
except Exception as e:
    print(f"Error analyzing attention patterns: {e}")
    print("Skipping detailed attention analysis.")
# Run attention analysis
analyze_attention_patterns(attention_examples)

# Function to compare attention in all models
def compare_model_attention():
    """Compare attention mechanisms in all three models"""
    print("\nComparing attention mechanisms:")
    print("1. Seq2Seq with Attention:")
    print("   - Uses single-head attention")
    print("   - Attention is between decoder hidden state and encoder outputs")
    print("   - Same attention weights used for all elements of the decoder output")

    print("\n2. Transformer:")
    print("   - Uses multi-head attention (splitting attention into multiple heads)")
    print("   - Self-attention in both encoder and decoder")
    print("   - Cross-attention between encoder and decoder")
    print("   - Separate attention weights for each position and each head")
    print("   - Incorporates positional encoding to capture sequence order")

    print("\nKey differences:")
    print("- Transformer processes all tokens in parallel during training")
    print("- Multi-head attention allows the model to attend to different representation subspaces")
    print("- Transformer uses layer normalization and residual connections")
    print("- Positional encoding in transformers vs. sequential processing in RNNs")

# Run comparison
compare_model_attention()

# Final summary
def summarize_findings():
    """Summarize key findings and conclusions"""
    print("\nSummary of findings:")
    print("1. Performance comparison:")
    print("   - Transformer achieves [better/worse/similar] performance compared to Seq2Seq with attention")
    print("   - Training time is [faster/slower] for Transformer")
    print("   - Model complexity (number of parameters) is [higher/lower] for Transformer")

    print("\n2. Qualitative analysis:")
    print("   - Transformer tends to generate [more coherent/diverse/fluent] headlines")
    print("   - Attention visualization shows [key patterns observed]")

    print("\n3. Limitations and future work:")
    print("   - Current implementation uses small model size due to computational constraints")
    print("   - Could be improved with pre-training, larger datasets, or beam search decoding")
    print("   - Further hyperparameter tuning could yield better results")

    print("\nConclusion:")
    print("The transformer architecture demonstrates the power of self-attention mechanisms for")
    print("sequence-to-sequence tasks like headline generation, with several advantages over")
    print("traditional RNN-based approaches. The ability to process sequences in parallel and")
    print("attend to different parts of the input simultaneously makes it particularly well-suited")
    print("for capturing complex dependencies in text data.")

# Run summary
summarize_findings()

print("\nTask 4: Transformer-based Headline Generation - Completed!")

!pip install tabulate

# Task 5: Analyze and Compare Models
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from tabulate import tabulate  # Make sure to install this package: pip install tabulate

# Define metrics variables for all models (use your actual values)
# For LSTM/GRU without attention (Seq2Seq)
seq2seq_metrics = {
    'BLEU-1': 0.0000,
    'BLEU-4': 0.0000,
    'ROUGE-L': 0.0000,

}
seq2seq_training_time = 340.52  # in seconds
seq2seq_inference_time = 9.29  # in seconds
seq2seq_params = 13384190  # approximate parameter count

# For Seq2Seq with attention
attention_metrics = {
    'BLEU-1': 0.0484,
    'BLEU-4': 0.0153,
    'ROUGE-L': 0.1496,

}
attention_training_time = 772.70  # in seconds
attention_inference_time = 44.19  # in seconds
attention_params = 27851614  # approximate parameter count

# For Transformer (use your actual values)
transformer_metrics = metrics  # This should be your transformer metrics from the previous code
transformer_training_time = training_time  # This should be your transformer training time from the previous code
transformer_inference_time = inference_time  # This should be your transformer inference time from the previous code
transformer_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)

# Create a comparison table
def create_comparison_table():
    """Create a detailed comparison table for all three models"""
    # Create pandas DataFrame for the comparison
    comparison_data = {
        'Criteria': [
            'BLEU-1', 'BLEU-4', 'ROUGE-L',
            'Training Time (s)', 'Inference Time (s)', 'Parameters',
            'Interpretability'
        ],
        'LSTM/GRU (No Attention)': [
            f"{seq2seq_metrics['BLEU-1']:.4f}",
            f"{seq2seq_metrics['BLEU-4']:.4f}",
            f"{seq2seq_metrics['ROUGE-L']:.4f}",
            f"{seq2seq_training_time:.2f}",
            f"{seq2seq_inference_time:.2f}",
            f"{seq2seq_params:,}",
            "Limited"
        ],
        'Attention (Bahdanau/Luong)': [
            f"{attention_metrics['BLEU-1']:.4f}",
            f"{attention_metrics['BLEU-4']:.4f}",
            f"{attention_metrics['ROUGE-L']:.4f}",
            f"{attention_training_time:.2f}",
            f"{attention_inference_time:.2f}",
            f"{attention_params:,}",
            "Good (Attention Maps)"
        ],
        'Transformer (Self-Attention)': [
            f"{transformer_metrics['BLEU-1']:.4f}",
            f"{transformer_metrics['BLEU-4']:.4f}",
            f"{transformer_metrics['ROUGE-L']:.4f}",
            f"{transformer_training_time:.2f}",
            f"{transformer_inference_time:.2f}",
            f"{transformer_params:,}",
            "Excellent (Multiple Attention Heads)"
        ]
    }

    df = pd.DataFrame(comparison_data)

    # Print the comparison table
    print("\nModel Comparison Table:")
    print("=" * 80)
    print(tabulate(df, headers='keys', tablefmt='pretty', showindex=False))
    print("=" * 80)

    return df

# Create bar charts for comparison
def plot_comparison_charts(df):
    """Create visual comparisons between models"""
    metrics_to_plot = ['BLEU-1', 'BLEU-4', 'ROUGE-L']

    # Convert string metrics back to float for plotting
    plot_data = df.copy()
    for metric in metrics_to_plot:
        for col in plot_data.columns[1:]:  # Skip 'Criteria' column
            plot_data.loc[plot_data['Criteria'] == metric, col] = float(df.loc[df['Criteria'] == metric, col].iloc[0])

    # Create subplots for each metric
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    axes = axes.flatten()

    # Models
    models = ['LSTM/GRU (No Attention)', 'Attention (Bahdanau/Luong)', 'Transformer (Self-Attention)']

    # Colors for each model
    colors = ['#3498db', '#2ecc71', '#e74c3c']

    for i, metric in enumerate(metrics_to_plot):
        metric_values = [float(plot_data.loc[plot_data['Criteria'] == metric, model].iloc[0]) for model in models]

        # Create bar chart
        bars = axes[i].bar(models, metric_values, color=colors)

        # Add value labels on top of each bar
        for bar in bars:
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.005,
                    f'{height:.4f}', ha='center', va='bottom', fontweight='bold')

        axes[i].set_title(f'{metric} Comparison', fontsize=14)
        axes[i].set_ylim(0, max(metric_values) * 1.2)  # Add some space for the text
        axes[i].grid(axis='y', linestyle='--', alpha=0.7)

        # Rotate x-axis labels if needed
        plt.setp(axes[i].get_xticklabels(), rotation=15, ha='right')

    plt.tight_layout()
    plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

    # Create a separate chart for time and complexity
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    # Training time
    training_times = [
        float(df.loc[df['Criteria'] == 'Training Time (s)', model].iloc[0])
        for model in models
    ]

    axes[0].bar(models, training_times, color=colors)
    axes[0].set_title('Training Time (seconds)', fontsize=14)
    axes[0].set_ylabel('Seconds')
    axes[0].grid(axis='y', linestyle='--', alpha=0.7)
    plt.setp(axes[0].get_xticklabels(), rotation=15, ha='right')

    # Model complexity (parameters)
    params = [
        int(df.loc[df['Criteria'] == 'Parameters', model].iloc[0].replace(',', ''))
        for model in models
    ]

    axes[1].bar(models, params, color=colors)
    axes[1].set_title('Model Complexity (Parameters)', fontsize=14)
    axes[1].set_ylabel('Number of Parameters')
    axes[1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
    axes[1].grid(axis='y', linestyle='--', alpha=0.7)
    plt.setp(axes[1].get_xticklabels(), rotation=15, ha='right')

    plt.tight_layout()
    plt.savefig('complexity_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# Create radar chart for overall comparison
def plot_radar_chart(df):
    """Create a radar chart for comparing models across dimensions"""
    # Select metrics to include in radar chart
    radar_metrics = ['BLEU-1', 'BLEU-4', 'ROUGE-L']

    # Convert string metrics back to float for plotting
    plot_data = df.copy()
    for metric in radar_metrics:
        for col in plot_data.columns[1:]:  # Skip 'Criteria' column
            plot_data.loc[plot_data['Criteria'] == metric, col] = float(df.loc[df['Criteria'] == metric, col].iloc[0])

    # Models
    models = ['LSTM/GRU (No Attention)', 'Attention (Bahdanau/Luong)', 'Transformer (Self-Attention)']

    # Extract values
    values = np.zeros((len(models), len(radar_metrics)))
    for i, model in enumerate(models):
        for j, metric in enumerate(radar_metrics):
            values[i, j] = float(plot_data.loc[plot_data['Criteria'] == metric, model].iloc[0])

    # Normalize values for better visualization
    max_values = values.max(axis=0)
    values_normalized = values / max_values

    # Create radar chart
    angles = np.linspace(0, 2*np.pi, len(radar_metrics), endpoint=False).tolist()
    angles += angles[:1]  # Close the loop

    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))

    # Add metrics to graph
    labels = radar_metrics + [radar_metrics[0]]  # Close the loop
    ax.set_thetagrids(np.degrees(angles), labels)

    # Colors
    colors = ['#3498db', '#2ecc71', '#e74c3c']

    for i, model in enumerate(models):
        values_plot = values_normalized[i].tolist()
        values_plot += values_plot[:1]  # Close the loop
        ax.plot(angles, values_plot, 'o-', linewidth=2, label=model, color=colors[i])
        ax.fill(angles, values_plot, alpha=0.1, color=colors[i])

    ax.set_title('Model Performance Comparison', size=15)
    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

    plt.tight_layout()
    plt.savefig('radar_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# Function to run all comparisons
def run_full_comparison():
    """Run all comparison analyses"""
    # Generate comparison table
    df = create_comparison_table()

    # Generate comparison charts
    try:
        plot_comparison_charts(df)
    except Exception as e:
        print(f"Error generating comparison charts: {e}")

    # Generate radar chart
    try:
        plot_radar_chart(df)
    except Exception as e:
        print(f"Error generating radar chart: {e}")

    # Write qualitative comparison
    print("\nQualitative Analysis:")
    print("=" * 80)
    print("1. Interpretability:")
    print("   - LSTM/GRU: Limited interpretability as hidden states are difficult to analyze")
    print("   - Attention: Good interpretability with attention maps showing which input tokens are important")
    print("   - Transformer: Excellent interpretability with multi-head attention showing different relationships")

    print("\n2. Generation Quality:")
    print("   - LSTM/GRU: Basic generation capability with sequential processing limitations")
    print("   - Attention: Improved coherence and relevance by focusing on important input parts")
    print("   - Transformer: Best overall generation with ability to capture long-range dependencies")

    print("\n3. Efficiency:")
    print("   - LSTM/GRU: Fastest training but limited parallelization")
    print("   - Attention: Moderate efficiency with added computation for attention")
    print("   - Transformer: Most computationally intensive but highly parallelizable")

    print("\n4. Model Complexity:")
    print("   - LSTM/GRU: Simplest architecture with fewest parameters")
    print("   - Attention: Moderate complexity with additional attention layers")
    print("   - Transformer: Most complex with multiple attention layers and feed-forward networks")

    print("\nOverall Recommendation:")
    print("Based on the comparative analysis, the Transformer architecture provides the best")
    print("balance of performance and interpretability for headline generation, though at the")
    print("cost of increased computational requirements. For resource-constrained environments,")
    print("the attention-based Seq2Seq model offers a good compromise.")

# Run the comparison
run_full_comparison()

